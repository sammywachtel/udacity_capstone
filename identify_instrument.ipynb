{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 20:11:46.598551 139828537165632 deprecation_wrapper.py:119] From /home/ec2-user/git/udacity_capstone/utils/general_utils.py:15: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import general_utils\n",
    "from utils import audio_utils\n",
    "import importlib\n",
    "importlib.reload(general_utils)\n",
    "import pandas as pd\n",
    "import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.general_utils' from '/home/ec2-user/git/udacity_capstone/utils/general_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(audio_utils)\n",
    "importlib.reload(general_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Neural Network to Predict Musical Instrument Family\n",
    "\n",
    "Goal: Use the NSynth Dataset by Google Inc. data to train a deep neural net to label the instrument playing a single note at any pitch or velocity.\n",
    "\n",
    "Instrument Families: Bass, Brass, Flute, Guitar, Keyboard, Mallet, Organ, Reed, String, Synth Lead, Vocal\n",
    "\n",
    "Dataset: https://magenta.tensorflow.org/datasets/nsynth\n",
    "\n",
    "Benchmark Model: Naive Predictor: Given the distribution of instrument samples (seen below), the chances of predicting any one instrument correctly at random are approximately 9.09%\n",
    "<pre>\n",
    "Instrument       Samples      Proportion of Dataset\n",
    "Bass              68,955             22.54%\n",
    "Brass             13,830              4.52%\n",
    "Flute              9,423              3.08%\n",
    "Guitar            35,423             11.58%\n",
    "Keyboard          54,991             17.97%\n",
    "Mallet            35,066             11.46%\n",
    "Organ             36,577             11.95%\n",
    "Reed              14,866              4.86%\n",
    "String            20,594              6.73%\n",
    "Synth Lead         5,501              1.80%\n",
    "Vocal             10,753              3.51%\n",
    "</pre>\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Extract all the sound samples from the NSynth dataset and generate spectrogram jpegs for each\n",
    "- Load the the spectograms into three labeled sets (training, validation, testing)\n",
    "- Train on a number of different models\n",
    "- Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a tfrecord file's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECORD  1 \n",
      "\n",
      "instrument_family_str\n",
      "bytes_list {\n",
      "  value: \"bass\"\n",
      "}\n",
      "\n",
      "velocity\n",
      "int64_list {\n",
      "  value: 100\n",
      "}\n",
      "\n",
      "pitch\n",
      "int64_list {\n",
      "  value: 100\n",
      "}\n",
      "\n",
      "instrument_family\n",
      "int64_list {\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "note_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033-100-100\"\n",
      "}\n",
      "\n",
      "sample_rate\n",
      "int64_list {\n",
      "  value: 16000\n",
      "}\n",
      "\n",
      "instrument_source\n",
      "int64_list {\n",
      "  value: 2\n",
      "}\n",
      "\n",
      "qualities\n",
      "int64_list {\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "instrument_source_str\n",
      "bytes_list {\n",
      "  value: \"synthetic\"\n",
      "}\n",
      "\n",
      "audio\n",
      "omitted\n",
      "\n",
      "instrument\n",
      "int64_list {\n",
      "  value: 417\n",
      "}\n",
      "\n",
      "qualities_str\n",
      "bytes_list {\n",
      "}\n",
      "\n",
      "note\n",
      "int64_list {\n",
      "  value: 149013\n",
      "}\n",
      "\n",
      "instrument_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033\"\n",
      "}\n",
      "\n",
      "\n",
      "RECORD  2 \n",
      "\n",
      "instrument_family\n",
      "int64_list {\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "note_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033-100-127\"\n",
      "}\n",
      "\n",
      "instrument_source\n",
      "int64_list {\n",
      "  value: 2\n",
      "}\n",
      "\n",
      "sample_rate\n",
      "int64_list {\n",
      "  value: 16000\n",
      "}\n",
      "\n",
      "qualities\n",
      "int64_list {\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "instrument_source_str\n",
      "bytes_list {\n",
      "  value: \"synthetic\"\n",
      "}\n",
      "\n",
      "audio\n",
      "omitted\n",
      "\n",
      "instrument\n",
      "int64_list {\n",
      "  value: 417\n",
      "}\n",
      "\n",
      "qualities_str\n",
      "bytes_list {\n",
      "}\n",
      "\n",
      "note\n",
      "int64_list {\n",
      "  value: 155292\n",
      "}\n",
      "\n",
      "instrument_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033\"\n",
      "}\n",
      "\n",
      "instrument_family_str\n",
      "bytes_list {\n",
      "  value: \"bass\"\n",
      "}\n",
      "\n",
      "velocity\n",
      "int64_list {\n",
      "  value: 127\n",
      "}\n",
      "\n",
      "pitch\n",
      "int64_list {\n",
      "  value: 100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "general_utils.list_data_in_tfrecord(\"data/nsynth-test.tfrecord\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Sound Samples to Spectrogram Images\n",
    "\n",
    "NOTE: Moved most of the logic for writing spectrograms is in the audio_utils library. Here's what it does:\n",
    "\n",
    "1. Load the Tensorflow recordset (tfrecord file) with three specicific features we need: note_str, audio, and sample_rate).\n",
    " - note_str: name of the note (brass_acoustic_059-062-050)\n",
    " - audio: array of floats (samples)\n",
    " - sample_rate: samples per second\n",
    "2. Use an asynchronous parallelized process to convert the sound files to spectograms in batches.\n",
    " - This function can:\n",
    " -  Write either mel scaled spectrograms or normally scaled spectrograms\n",
    " -  Overwrite files or not\n",
    " -  Use a regex filter to limit the dataset based on sound name patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only note '64'\n",
    "\n",
    "test_spectrogram_folder = 'data/nsynth-test-mel-spectrograms-064'\n",
    "valid_spectrogram_folder = 'data/nsynth-valid-mel-spectrograms-064'\n",
    "train_spectrogram_folder = 'data/nsynth-train-mel-spectrograms-064'\n",
    "\n",
    "#regex_filter = None\n",
    "regex_filter = '.*-064-.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "processed 50 files out of 61 in 1 batches\r",
      "processed 61 files out of 61 in 1 batches"
     ]
    }
   ],
   "source": [
    "importlib.reload(audio_utils)\n",
    "\n",
    "audio_utils.write_spectograms_parallelized('data/nsynth-test.tfrecord', test_spectrogram_folder, \n",
    "                                           batch_size=200, mel=True, overwrite=False, regex_filter=regex_filter)\n",
    "\n",
    "audio_utils.write_spectograms_parallelized('data/nsynth-valid.tfrecord', valid_spectrogram_folder, \n",
    "                                           batch_size=200, mel=True, overwrite=False, regex_filter=regex_filter)\n",
    "\n",
    "## following took about 4 or 5 hours with full dataset\n",
    "audio_utils.write_spectograms_parallelized('data/nsynth-train.tfrecord', train_spectrogram_folder, \n",
    "                                           batch_size=200, mel=True, overwrite=False, regex_filter=regex_filter)\n",
    "\n",
    "#output for full dataset was: \"processed 289205 files out of 289205 in 1446 batches\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets (files, targets, and names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info about training set\n",
      "number of samples:  4035\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n",
      "\n",
      "info about validation set\n",
      "number of samples:  160\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n",
      "\n",
      "info about test set\n",
      "number of samples:  61\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n"
     ]
    }
   ],
   "source": [
    "train_files, train_targets, train_target_names = general_utils.load_dataset(train_spectrogram_folder)\n",
    "\n",
    "print('info about training set')\n",
    "print('number of samples: ', len(train_files))\n",
    "print('categories are: ', sorted(set(train_target_names)))\n",
    "#print(train_files[500:501])\n",
    "#print(train_targets[500:501])\n",
    "#print(train_target_names[500:501])\n",
    "\n",
    "valid_files, valid_targets, valid_target_names = general_utils.load_dataset(valid_spectrogram_folder)\n",
    "\n",
    "print('\\ninfo about validation set')\n",
    "print('number of samples: ', len(valid_files))\n",
    "print('categories are: ', sorted(set(valid_target_names)))\n",
    "#print(valid_files[500:501])\n",
    "#print(valid_targets[500:501])\n",
    "#print(valid_target_names[500:501])\n",
    "\n",
    "# load the test data\n",
    "test_files, test_targets, test_target_names = general_utils.load_dataset(test_spectrogram_folder)\n",
    "\n",
    "print('\\ninfo about test set')\n",
    "print('number of samples: ', len(test_files))\n",
    "print('categories are: ', sorted(set(test_target_names)))\n",
    "#print(test_files[500:501])\n",
    "#print(test_targets[500:501])\n",
    "#print(test_target_names[500:501])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataframes\n",
    "build dataframes that will be part of the image generator construction\n",
    "\n",
    "### PART 1: zip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the three arrays into an array of tupples\n",
    "train_data = list(zip(train_files, train_targets, train_target_names))\n",
    "valid_data = list(zip(valid_files, valid_targets, valid_target_names))\n",
    "test_data = list(zip(test_files, test_targets, test_target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Limit the size of the datasets if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment / change the code here\n",
    "\n",
    "#train_data = train_data[:50000]\n",
    "#valid_data = valid_data[:2150]\n",
    "#test_data = test_data[:4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: Create the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns = ['file_paths', 'targets', 'target_names'])\n",
    "valid_df = pd.DataFrame(valid_data, columns = ['file_paths', 'targets', 'target_names'])\n",
    "test_df = pd.DataFrame(test_data, columns = ['file_paths', 'targets', 'target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### train shape (4035, 3)\n",
      "### valid shape (160, 3)\n",
      "### test shape (61, 3)\n"
     ]
    }
   ],
   "source": [
    "print('### train shape', train_df.shape)\n",
    "print('### valid shape', valid_df.shape)\n",
    "print('### test shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ImageDataGenerators\n",
    "\n",
    "This Keras tool generates batches of tensor image data. It can augment the images as well, but for this project I have not used that feature as spectograms are not 'in the wild' as photographs tend to be. I would be interesting to experiment with this in a further iteration, however.\n",
    "\n",
    "One generator for each of the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data generator\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen=ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4035 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the training data generator\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(128,128)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the validation data generator\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(128,128)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the test data generator\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(128,128)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFULL THINGS\n",
    "\n",
    "#to go through all of the files to make sure they're findable\n",
    "#import os.path\n",
    "#for f in valid_files:#valid_generator.filepaths:\n",
    "#    if not os.path.exists(f):\n",
    "#        print(f, 'does not exist')\n",
    "\n",
    "# get all of the file paths in the data generator\n",
    "#generator_files = set(valid_generator.filepaths)\n",
    "\n",
    "# make sure all of the actual files are accounted for in the generator\n",
    "#i = 0\n",
    "#for f in valid_files:\n",
    "#    if f not in generator_files:\n",
    "#        print('N', f)\n",
    "#        i+=1\n",
    "    #else:\n",
    "        #print('Y', f)\n",
    "#print(i, 'files do not match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the step sizes for each of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_generator.n//train_generator.batch_size is 4035//32\n",
      "valid_generator.n//valid_generator.batch_size is 160//32\n",
      "test_generator.n//test_generator.batch_size is 59//32\n",
      "train step size: 127, validation step size: 6, test step size: 2\n"
     ]
    }
   ],
   "source": [
    "steps_train=train_generator.n//train_generator.batch_size + 1\n",
    "steps_valid=valid_generator.n//valid_generator.batch_size + 1\n",
    "steps_test=test_generator.n//test_generator.batch_size + 1\n",
    "#STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "print('train_generator.n//train_generator.batch_size is {}//{}'.format(train_generator.n, train_generator.batch_size))\n",
    "print('valid_generator.n//valid_generator.batch_size is {}//{}'.format(valid_generator.n, valid_generator.batch_size))\n",
    "print('test_generator.n//test_generator.batch_size is {}//{}'.format(test_generator.n, test_generator.batch_size))\n",
    "print('train step size: {}, validation step size: {}, test step size: {}'.format(steps_train, steps_valid, steps_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models\n",
    "\n",
    "This cell is reused for each tested model. The models are all added to the 'models' library and labeled with a number. See the noted results of each model in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models)\n",
    "\n",
    "# Save the checkpoints to here. Make sure the file name reflects the model version.\n",
    "model_hdf5 = 'saved_models/weights.best.v10_1.hdf5'\n",
    "\n",
    "# Use the same model for each of the steps below. See 'models.py' for details.\n",
    "model_creator = models.create_model_v10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 126, 126, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 63, 63, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 63, 63, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 63, 63, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 61, 61, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 61, 61, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               58982912  \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 59,450,314\n",
      "Trainable params: 59,450,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 1875.2564 - acc: 0.2773\n",
      "Epoch 00001: val_loss improved from inf to 1.92108, saving model to saved_models/weights.best.v10_1.hdf5\n",
      "127/127 [==============================] - 49s 389ms/step - loss: 1860.6096 - acc: 0.2781 - val_loss: 1.9211 - val_acc: 0.2812\n",
      "Epoch 2/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 3.9605 - acc: 0.2368\n",
      "Epoch 00002: val_loss did not improve from 1.92108\n",
      "127/127 [==============================] - 42s 331ms/step - loss: 3.9466 - acc: 0.2362 - val_loss: 2.1997 - val_acc: 0.1562\n",
      "Epoch 3/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 2.2062 - acc: 0.1784\n",
      "Epoch 00003: val_loss did not improve from 1.92108\n",
      "127/127 [==============================] - 42s 331ms/step - loss: 2.2068 - acc: 0.1779 - val_loss: 2.1928 - val_acc: 0.1667\n",
      "Epoch 4/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 2.2074 - acc: 0.1744\n",
      "Epoch 00004: val_loss did not improve from 1.92108\n",
      "127/127 [==============================] - 42s 332ms/step - loss: 2.2082 - acc: 0.1745 - val_loss: 2.1862 - val_acc: 0.1458\n",
      "Epoch 5/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 2.2042 - acc: 0.1716\n",
      "Epoch 00005: val_loss did not improve from 1.92108\n",
      "127/127 [==============================] - 42s 332ms/step - loss: 2.2044 - acc: 0.1715 - val_loss: 2.1905 - val_acc: 0.1562\n",
      "Epoch 6/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 2.2056 - acc: 0.1674\n",
      "Epoch 00006: val_loss did not improve from 1.92108\n",
      "127/127 [==============================] - 42s 331ms/step - loss: 2.2060 - acc: 0.1680 - val_loss: 2.2050 - val_acc: 0.1458\n",
      "Epoch 7/30\n",
      "126/127 [============================>.] - ETA: 0s - loss: 2.2030 - acc: 0.1641\n",
      "Epoch 00007: val_loss did not improve from 1.92108\n",
      "127/127 [==============================] - 42s 331ms/step - loss: 2.2021 - acc: 0.1643 - val_loss: 2.1894 - val_acc: 0.1510\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=model_hdf5, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# End trining if there's no improvement in four epochs.\n",
    "early_stopper = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
    "\n",
    "train_model = model_creator(show_summary=True)\n",
    "train_model.fit_generator(generator=train_generator,\n",
    "                          steps_per_epoch=steps_train,\n",
    "                          validation_data=valid_generator,\n",
    "                          validation_steps=steps_valid,\n",
    "                          epochs=30,\n",
    "                          callbacks=[checkpointer,early_stopper],\n",
    "                          workers=4\n",
    ")\n",
    "\n",
    "r_trained_orig = train_model.evaluate_generator(generator=valid_generator, steps=steps_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "NOTE: The results below are ONLY for the most recently tested model.\n",
    "\n",
    "<b> Please see the `models.py` file for all of the models and the results for each.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Accuracy of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loss:  2.185720960299174  Accuracy:  0.15104167\n"
     ]
    }
   ],
   "source": [
    "#print(train_model.metrics_names)\n",
    "#print(r_trained_orig)\n",
    "print('## Loss: ', r_trained_orig[0], ' Accuracy: ', r_trained_orig[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Untrained Model\n",
    "### Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loss:  5.309322357177734  Accuracy:  0.016393442\n"
     ]
    }
   ],
   "source": [
    "# first test untrained model\n",
    "untrained_test_model = model_creator()\n",
    "\n",
    "score_untrained = untrained_test_model.evaluate_generator(test_generator, steps=steps_test)\n",
    "print('## Loss: ', score_untrained[0], ' Accuracy: ', score_untrained[1])\n",
    "\n",
    "#for i, n in enumerate(test_generator.filenames):\n",
    "#    print('file:', n, ' score: ', scores[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Total test records: 61\n",
      "## Number of correct: 4\n",
      "## Percent correct: 0.06557377049180328\n"
     ]
    }
   ],
   "source": [
    "## Testing the untrained model\n",
    "importlib.reload(general_utils)\n",
    "\n",
    "results = general_utils.run_prediction(untrained_test_model, test_generator, steps_test)\n",
    "\n",
    "num_untrained_test_correct = np.count_nonzero(results)\n",
    "\n",
    "print('## Total test records:', len(results))\n",
    "print('## Number of correct:', num_untrained_test_correct)\n",
    "print('## Percent correct:', num_untrained_test_correct/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Trained Model\n",
    "### Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loss:  1.866533100605011  Accuracy:  0.36065573\n"
     ]
    }
   ],
   "source": [
    "# next test the trained model\n",
    "trained_test_model = model_creator()\n",
    "trained_test_model.load_weights(model_hdf5)\n",
    "\n",
    "score_trained = trained_test_model.evaluate_generator(test_generator, steps=steps_test)\n",
    "print('## Loss: ', score_trained[0], ' Accuracy: ', score_trained[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models/weights.best.v10_1.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(model_hdf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of the Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Total test records: 61\n",
      "## Number of correct: 12\n",
      "## Percent correct: 0.19672131147540983\n"
     ]
    }
   ],
   "source": [
    "## Testing the trained model\n",
    "importlib.reload(general_utils)\n",
    "\n",
    "results = general_utils.run_prediction(trained_test_model, test_generator, steps_test)\n",
    "\n",
    "num_trained_test_correct = np.count_nonzero(results)\n",
    "print('## Total test records:', len(results))\n",
    "print('## Number of correct:', num_trained_test_correct)\n",
    "print('## Percent correct:', num_trained_test_correct/len(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
