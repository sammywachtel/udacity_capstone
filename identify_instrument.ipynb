{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Training\n",
    "- Extract all the samples and generate spectrogram jpegs for each\n",
    "- Train a CNN on the jpegs\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import general_utils\n",
    "import importlib\n",
    "importlib.reload(general_utils)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info about training set\n",
      "number of samples:  283704\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n",
      "\n",
      "info about validation set\n",
      "number of samples:  12678\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n",
      "\n",
      "info about test set\n",
      "number of samples:  4096\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_files, train_targets, train_target_names = general_utils.load_dataset('data/nsynth-train-spectrograms')\n",
    "\n",
    "print('info about training set')\n",
    "print('number of samples: ', len(train_files))\n",
    "print('categories are: ', sorted(set(train_target_names)))\n",
    "#print(train_files[500:501])\n",
    "#print(train_targets[500:501])\n",
    "#print(train_target_names[500:501])\n",
    "\n",
    "valid_files, valid_targets, valid_target_names = general_utils.load_dataset('data/nsynth-valid-spectrograms')\n",
    "\n",
    "print('\\ninfo about validation set')\n",
    "print('number of samples: ', len(valid_files))\n",
    "print('categories are: ', sorted(set(valid_target_names)))\n",
    "#print(valid_files[500:501])\n",
    "#print(valid_targets[500:501])\n",
    "#print(valid_target_names[500:501])\n",
    "\n",
    "# load the test data\n",
    "test_files, test_targets, test_target_names = general_utils.load_dataset('data/nsynth-test-spectrograms')\n",
    "\n",
    "print('\\ninfo about test set')\n",
    "print('number of samples: ', len(test_files))\n",
    "print('categories are: ', sorted(set(test_target_names)))\n",
    "#print(test_files[500:501])\n",
    "#print(test_targets[500:501])\n",
    "#print(test_target_names[500:501])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataframe that will be part of the image generator construction\n",
    "#PART 1: zip the data\n",
    "\n",
    "# merge the three arrays into an array of tupples\n",
    "train_data = list(zip(train_files, train_targets, train_target_names))\n",
    "valid_data = list(zip(valid_files, valid_targets, valid_target_names))\n",
    "test_data = list(zip(test_files, test_targets, test_target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: to limit the size of the datasets, uncomment / change the code here\n",
    "\n",
    "train_data = train_data[:100000]\n",
    "valid_data = valid_data[:4300]\n",
    "#test_data = test_data[:4300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Create the dataframes\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns = ['file_paths', 'targets', 'target_names'])\n",
    "valid_df = pd.DataFrame(valid_data, columns = ['file_paths', 'targets', 'target_names'])\n",
    "test_df = pd.DataFrame(test_data, columns = ['file_paths', 'targets', 'target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (100000, 3)\n",
      "valid shape (4300, 3)\n",
      "test shape (4096, 3)\n"
     ]
    }
   ],
   "source": [
    "print('train shape', train_df.shape)\n",
    "print('valid shape', valid_df.shape)\n",
    "print('test shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data generator\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the training data generator\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(64,64)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4300 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the validation data generator\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(64,64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4096 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the validation data generator\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        #seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(64,64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFULL THINGS\n",
    "\n",
    "#to go through all of the files to make sure they're finable\n",
    "#import os.path\n",
    "#for f in valid_files:#valid_generator.filepaths:\n",
    "#    if not os.path.exists(f):\n",
    "#        print(f, 'does not exist')\n",
    "\n",
    "# get all of the file paths in the data generator\n",
    "#generator_files = set(valid_generator.filepaths)\n",
    "\n",
    "# make sure all of the actual files are accounted for in the generator\n",
    "#i = 0\n",
    "#for f in valid_files:\n",
    "#    if f not in generator_files:\n",
    "#        print('N', f)\n",
    "#        i+=1\n",
    "    #else:\n",
    "        #print('Y', f)\n",
    "#print(i, 'files do not match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://medium.com/gradientcrescent/urban-sound-classification-using-convolutional-neural-networks-with-keras-theory-and-486e92785df4\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_model(show_summary=False):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(64,64,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizers.RMSprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_generator.n//train_generator.batch_size is 100000//32\n",
      "valid_generator.n//valid_generator.batch_size is 4300//32\n",
      "test_generator.n//test_generator.batch_size is 4096//32\n",
      "STEP_SIZE_TRAIN, STEP_SIZE_VALID, STEP_SIZE_TEST is 3125, 134, 128\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "#STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "print('train_generator.n//train_generator.batch_size is {}//{}'.format(train_generator.n, train_generator.batch_size))\n",
    "print('valid_generator.n//valid_generator.batch_size is {}//{}'.format(valid_generator.n, valid_generator.batch_size))\n",
    "print('test_generator.n//test_generator.batch_size is {}//{}'.format(test_generator.n, test_generator.batch_size))\n",
    "print('STEP_SIZE_TRAIN, STEP_SIZE_VALID, STEP_SIZE_TEST is {}, {}, {}'.format(STEP_SIZE_TRAIN, STEP_SIZE_VALID, STEP_SIZE_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.v2.hdf5', \n",
    "#                               verbose=1, save_best_only=True)\n",
    "\n",
    "#train_model = create_model(show_summary=True)\n",
    "#train_model.fit_generator(generator=train_generator,\n",
    "#                          steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                          validation_data=valid_generator,\n",
    "#                          validation_steps=STEP_SIZE_VALID,\n",
    "#                          epochs=10,\n",
    "#                          callbacks=[checkpointer]\n",
    "#)\n",
    "\n",
    "#r_trained_orig = train_model.evaluate_generator(generator=valid_generator, steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 22:46:37.797879 140058644711232 deprecation.py:506] From /home/ec2-user/anaconda3/envs/magenta-gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.3010107818990946  Accuracy:  0.036621094\n"
     ]
    }
   ],
   "source": [
    "# first test untrained model\n",
    "untrained_test_model = create_model()\n",
    "\n",
    "score_untrained = untrained_test_model.evaluate_generator(test_generator, steps=STEP_SIZE_TEST)\n",
    "print('Loss: ', score_untrained[0], ' Accuracy: ', score_untrained[1])\n",
    "\n",
    "#for i, n in enumerate(test_generator.filenames):\n",
    "#    print('file:', n, ' score: ', scores[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test records: 4096\n",
      "Number of correct: 185\n"
     ]
    }
   ],
   "source": [
    "## Testing the untrained model\n",
    "importlib.reload(general_utils)\n",
    "\n",
    "results = general_utils.run_prediction(untrained_test_model, test_generator, STEP_SIZE_TEST)\n",
    "\n",
    "print('Total test records:', len(results))\n",
    "print('Number of correct:', np.count_nonzero(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.9580376138910651  Accuracy:  0.6879883\n"
     ]
    }
   ],
   "source": [
    "# next test the trained model\n",
    "trained_test_model = create_model()\n",
    "trained_test_model.load_weights('saved_models/weights.best.v1.hdf5')\n",
    "\n",
    "score_trained = trained_test_model.evaluate_generator(test_generator, steps=STEP_SIZE_TEST)\n",
    "print('Loss: ', score_trained[0], ' Accuracy: ', score_trained[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test records: 4096\n",
      "Number of correct: 566\n"
     ]
    }
   ],
   "source": [
    "## Testing the trained model\n",
    "importlib.reload(general_utils)\n",
    "\n",
    "results = general_utils.run_prediction(trained_test_model, test_generator, STEP_SIZE_TEST)\n",
    "\n",
    "print('Total test records:', len(results))\n",
    "print('Number of correct:', np.count_nonzero(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all possible labels: {0: 'bass', 1: 'brass', 2: 'flute', 3: 'guitar', 4: 'keyboard', 5: 'mallet', 6: 'organ', 7: 'reed', 8: 'string', 9: 'vocal'}\n",
      "True original label [0, 'bass'] \tfilename bass_synthetic_098-049-025.jpg \tpredicted label [0, 'bass']\n",
      "False original label [4, 'keyboard'] \tfilename keyboard_acoustic_004-033-127.jpg \tpredicted label [1, 'brass']\n",
      "False original label [4, 'keyboard'] \tfilename keyboard_acoustic_004-031-050.jpg \tpredicted label [5, 'mallet']\n",
      "False original label [6, 'organ'] \tfilename organ_electronic_028-101-050.jpg \tpredicted label [4, 'keyboard']\n",
      "False original label [8, 'string'] \tfilename string_acoustic_057-060-100.jpg \tpredicted label [4, 'keyboard']\n",
      "False original label [4, 'keyboard'] \tfilename keyboard_electronic_002-053-127.jpg \tpredicted label [5, 'mallet']\n",
      "True original label [0, 'bass'] \tfilename bass_synthetic_098-048-025.jpg \tpredicted label [0, 'bass']\n",
      "False original label [8, 'string'] \tfilename string_acoustic_056-055-100.jpg \tpredicted label [4, 'keyboard']\n",
      "False original label [6, 'organ'] \tfilename organ_electronic_113-070-100.jpg \tpredicted label [2, 'flute']\n",
      "True original label [0, 'bass'] \tfilename bass_synthetic_033-048-100.jpg \tpredicted label [0, 'bass']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### to display some details about the above results\n",
    "\n",
    "data_labels_dict = dict((data_labels[k],k) for k in data_labels.keys())\n",
    "print('all possible labels:',data_labels_dict)\n",
    "\n",
    "##### map the predictions to the labels for later display\n",
    "test_labels_pred = [[i,data_labels_dict[i]] for i in test_y_pred]\n",
    "test_labels_orig = [[i,data_labels_dict[i]] for i in test_y_labels]\n",
    "\n",
    "test_filenames = [f.split('/')[-1] for f in test_generator.filenames]\n",
    "[print(v[3],'original label',v[0],'\\tfilename',v[1],'\\tpredicted label',v[2]) for v in zip(test_labels_orig[:10], test_filenames[:10],test_labels_pred[:10],results[:10])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 19:18:24.825218 140180489463616 deprecation.py:323] From /home/ec2-user/anaconda3/envs/magenta-gpu/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py:1200: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Streaming restore not supported from name-based checkpoints when graph building. File a feature request if this limitation bothers you. As a workaround, consider either using tf.train.Checkpoint to load name-based checkpoints or enabling eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2013bf138518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrained_nsynth_test_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/magenta-gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    160\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    161\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/magenta-gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0;31m# Restore existing variables (if any) immediately, and set up a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;31m# streaming restore for any variables created in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m         \u001b[0mtrackable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m       \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/magenta-gpu/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mstreaming_restore\u001b[0;34m(status, session)\u001b[0m\n\u001b[1;32m    617\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNameBasedSaverStatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     raise NotImplementedError(\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;34m\"Streaming restore not supported from name-based checkpoints when \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;34m\"graph building. File a feature request if this limitation bothers \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;34m\"you. As a workaround, consider either using tf.train.Checkpoint to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Streaming restore not supported from name-based checkpoints when graph building. File a feature request if this limitation bothers you. As a workaround, consider either using tf.train.Checkpoint to load name-based checkpoints or enabling eager execution."
     ]
    }
   ],
   "source": [
    "# next test the model trained by NSynth (think I'll have to use a transfer learning approach)\n",
    "\n",
    "###NOTE: Could not figure out how to use the ckpt checkpoint files to either evaluate or even use \n",
    "### . as the basis for transfer learning. I'd like to go back and do this, but could use some advice.\n",
    "\n",
    "trained_nsynth_test_model = create_model()\n",
    "#nsynth_checkpoint = tf.train.load_checkpoint('examples/model_files/model.ckpt-200000') #, latest_filename='model.ckpt-200000.index'\n",
    "#print(nsynth_checkpoint)\n",
    "\n",
    "### results of this was:\n",
    "## NotImplementedError: Streaming restore not supported from name-based checkpoints when graph building. \n",
    "##   File a feature request if this limitation bothers you. \n",
    "##   As a workaround, consider either using tf.train.Checkpoint to load \n",
    "##   name-based checkpoints or enabling eager execution.\n",
    "\n",
    "## I did find this. See link in the comment: https://github.com/tensorflow/magenta/issues/955\n",
    "\n",
    "path = 'examples/model_files/model.ckpt-200000'\n",
    "#cp = tf.train.Checkpoint(path)\n",
    "\n",
    "\n",
    "trained_nsynth_test_model.load_weights(path)\n",
    "\n",
    "\n",
    "r_trained = trained_nsynth_test_model.evaluate_generator(test_generator, steps=STEP_SIZE_TEST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the results of a few\n",
    "p = untrained_test_model.predict_generator(test_generator, steps=STEP_SIZE_TEST, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and display the first 10 predictions\n",
    "p_indices = np.argmax(p, axis=1)\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "\n",
    "predictions = [labels[k] for k in p_indices]\n",
    "\n",
    "print(predictions[0:9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
