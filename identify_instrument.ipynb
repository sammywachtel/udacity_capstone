{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import general_utils\n",
    "from utils import audio_utils\n",
    "import importlib\n",
    "importlib.reload(general_utils)\n",
    "import pandas as pd\n",
    "import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.general_utils' from '/home/ec2-user/git/udacity_capstone/utils/general_utils.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(audio_utils)\n",
    "importlib.reload(general_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Neural Network to Predict Musical Instrument Family\n",
    "\n",
    "Goal: Use the NSynth Dataset by Google Inc. data to train a deep neural net to label the instrument playing a single note at any pitch or velocity.\n",
    "\n",
    "Instrument Families: Bass, Brass, Flute, Guitar, Keyboard, Mallet, Organ, Reed, String, Synth Lead, Vocal\n",
    "\n",
    "Dataset: https://magenta.tensorflow.org/datasets/nsynth\n",
    "\n",
    "Benchmark Model: Naive Predictor: Given the distribution of instrument samples (seen below), the chances of predicting any one instrument correctly at random are approximately 9.09%\n",
    "<pre>\n",
    "Instrument       Samples      Proportion of Dataset\n",
    "Bass              68,955             22.54%\n",
    "Brass             13,830              4.52%\n",
    "Flute              9,423              3.08%\n",
    "Guitar            35,423             11.58%\n",
    "Keyboard          54,991             17.97%\n",
    "Mallet            35,066             11.46%\n",
    "Organ             36,577             11.95%\n",
    "Reed              14,866              4.86%\n",
    "String            20,594              6.73%\n",
    "Synth Lead         5,501              1.80%\n",
    "Vocal             10,753              3.51%\n",
    "</pre>\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Extract all the sound samples from the NSynth dataset and generate spectrogram jpegs for each\n",
    "- Load the the spectograms into three labeled sets (training, validation, testing)\n",
    "- Train on a number of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a tfrecord file's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RECORD  1 \n",
      "\n",
      "qualities\n",
      "int64_list {\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "instrument_source_str\n",
      "bytes_list {\n",
      "  value: \"synthetic\"\n",
      "}\n",
      "\n",
      "audio\n",
      "omitted\n",
      "\n",
      "instrument\n",
      "int64_list {\n",
      "  value: 417\n",
      "}\n",
      "\n",
      "qualities_str\n",
      "bytes_list {\n",
      "}\n",
      "\n",
      "note\n",
      "int64_list {\n",
      "  value: 149013\n",
      "}\n",
      "\n",
      "instrument_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033\"\n",
      "}\n",
      "\n",
      "instrument_family_str\n",
      "bytes_list {\n",
      "  value: \"bass\"\n",
      "}\n",
      "\n",
      "velocity\n",
      "int64_list {\n",
      "  value: 100\n",
      "}\n",
      "\n",
      "pitch\n",
      "int64_list {\n",
      "  value: 100\n",
      "}\n",
      "\n",
      "instrument_family\n",
      "int64_list {\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "note_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033-100-100\"\n",
      "}\n",
      "\n",
      "sample_rate\n",
      "int64_list {\n",
      "  value: 16000\n",
      "}\n",
      "\n",
      "instrument_source\n",
      "int64_list {\n",
      "  value: 2\n",
      "}\n",
      "\n",
      "\n",
      "RECORD  2 \n",
      "\n",
      "velocity\n",
      "int64_list {\n",
      "  value: 127\n",
      "}\n",
      "\n",
      "pitch\n",
      "int64_list {\n",
      "  value: 100\n",
      "}\n",
      "\n",
      "instrument_family\n",
      "int64_list {\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "note_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033-100-127\"\n",
      "}\n",
      "\n",
      "instrument_source\n",
      "int64_list {\n",
      "  value: 2\n",
      "}\n",
      "\n",
      "sample_rate\n",
      "int64_list {\n",
      "  value: 16000\n",
      "}\n",
      "\n",
      "qualities\n",
      "int64_list {\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "}\n",
      "\n",
      "instrument_source_str\n",
      "bytes_list {\n",
      "  value: \"synthetic\"\n",
      "}\n",
      "\n",
      "audio\n",
      "omitted\n",
      "\n",
      "instrument\n",
      "int64_list {\n",
      "  value: 417\n",
      "}\n",
      "\n",
      "qualities_str\n",
      "bytes_list {\n",
      "}\n",
      "\n",
      "note\n",
      "int64_list {\n",
      "  value: 155292\n",
      "}\n",
      "\n",
      "instrument_str\n",
      "bytes_list {\n",
      "  value: \"bass_synthetic_033\"\n",
      "}\n",
      "\n",
      "instrument_family_str\n",
      "bytes_list {\n",
      "  value: \"bass\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "general_utils.list_data_in_tfrecord(\"data/nsynth-test.tfrecord\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Sound Samples to Spectogram Images\n",
    "\n",
    "NOTE: I moved most of the code that was in this cell to the audio_utils library to keep the code clean. Here's what it does:\n",
    "\n",
    "1. Load the Tensorflow recordset (tfrecord file) with two specicific features we need: note_str and audio).\n",
    "    note_str contains the name of the note (brass_acoustic_059-062-050) and audio contains an array of floats.\n",
    "2. Use an asynchronous parallelized process to convert the sound files to spectograms in batches.\n",
    "\n",
    "TODO: Because sklearn.datasets.load_files uses subdirectory names as labels, I had to move the image files after they'd been written to disk. For next iteration, have each file automatically get written into a subdirectory that matches the label name so the manual moving of files isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_utils.write_spectograms_parallelized('data/nsynth-test.tfrecord', 'data/nsynth-test-spectrograms', 200)\n",
    "audio_utils.write_spectograms_parallelized('data/nsynth-valid.tfrecord', 'data/nsynth-valid-spectrograms', 200)\n",
    "\n",
    "## following took about 4 or 5 hours\n",
    "audio_utils.write_spectograms_parallelized('data/nsynth-train.tfrecord', 'data/nsynth-train-spectrograms', 200)\n",
    "\n",
    "#output was: \"processed 289200 files out of 289205 in 1445 batches\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets (files, targets, and names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info about training set\n",
      "number of samples:  283704\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n",
      "\n",
      "info about validation set\n",
      "number of samples:  12678\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n",
      "\n",
      "info about test set\n",
      "number of samples:  4096\n",
      "categories are:  ['bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal']\n"
     ]
    }
   ],
   "source": [
    "train_files, train_targets, train_target_names = general_utils.load_dataset('data/nsynth-train-spectrograms')\n",
    "\n",
    "print('info about training set')\n",
    "print('number of samples: ', len(train_files))\n",
    "print('categories are: ', sorted(set(train_target_names)))\n",
    "#print(train_files[500:501])\n",
    "#print(train_targets[500:501])\n",
    "#print(train_target_names[500:501])\n",
    "\n",
    "valid_files, valid_targets, valid_target_names = general_utils.load_dataset('data/nsynth-valid-spectrograms')\n",
    "\n",
    "print('\\ninfo about validation set')\n",
    "print('number of samples: ', len(valid_files))\n",
    "print('categories are: ', sorted(set(valid_target_names)))\n",
    "#print(valid_files[500:501])\n",
    "#print(valid_targets[500:501])\n",
    "#print(valid_target_names[500:501])\n",
    "\n",
    "# load the test data\n",
    "test_files, test_targets, test_target_names = general_utils.load_dataset('data/nsynth-test-spectrograms')\n",
    "\n",
    "print('\\ninfo about test set')\n",
    "print('number of samples: ', len(test_files))\n",
    "print('categories are: ', sorted(set(test_target_names)))\n",
    "#print(test_files[500:501])\n",
    "#print(test_targets[500:501])\n",
    "#print(test_target_names[500:501])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataframes\n",
    "build dataframes that will be part of the image generator construction\n",
    "\n",
    "### PART 1: zip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the three arrays into an array of tupples\n",
    "train_data = list(zip(train_files, train_targets, train_target_names))\n",
    "valid_data = list(zip(valid_files, valid_targets, valid_target_names))\n",
    "test_data = list(zip(test_files, test_targets, test_target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Limit the size of the datasets if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment / change the code here\n",
    "\n",
    "#train_data = train_data[:50000]\n",
    "#valid_data = valid_data[:2150]\n",
    "#test_data = test_data[:4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: Create the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns = ['file_paths', 'targets', 'target_names'])\n",
    "valid_df = pd.DataFrame(valid_data, columns = ['file_paths', 'targets', 'target_names'])\n",
    "test_df = pd.DataFrame(test_data, columns = ['file_paths', 'targets', 'target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### train shape (283704, 3)\n",
      "### valid shape (12678, 3)\n",
      "### test shape (4096, 3)\n"
     ]
    }
   ],
   "source": [
    "print('### train shape', train_df.shape)\n",
    "print('### valid shape', valid_df.shape)\n",
    "print('### test shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ImageDataGenerators\n",
    "\n",
    "This Keras tool generates batches of tensor image data. It can augment the images as well, but for this project I have not used that feature as spectograms are not 'in the wild' as photographs tend to be. I would be interesting to experiment with this in a further iteration, however.\n",
    "\n",
    "One generator for each of the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data generator\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 283704 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the training data generator\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(64,64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12678 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the validation data generator\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(64,64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4096 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# make the validation data generator\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='file_paths',\n",
    "        y_col='target_names',\n",
    "        batch_size=32,\n",
    "        #seed=69,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical',\n",
    "        target_size=(64,64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFULL THINGS\n",
    "\n",
    "#to go through all of the files to make sure they're findable\n",
    "#import os.path\n",
    "#for f in valid_files:#valid_generator.filepaths:\n",
    "#    if not os.path.exists(f):\n",
    "#        print(f, 'does not exist')\n",
    "\n",
    "# get all of the file paths in the data generator\n",
    "#generator_files = set(valid_generator.filepaths)\n",
    "\n",
    "# make sure all of the actual files are accounted for in the generator\n",
    "#i = 0\n",
    "#for f in valid_files:\n",
    "#    if f not in generator_files:\n",
    "#        print('N', f)\n",
    "#        i+=1\n",
    "    #else:\n",
    "        #print('Y', f)\n",
    "#print(i, 'files do not match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the step sizes for each of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_generator.n//train_generator.batch_size is 283704//32\n",
      "valid_generator.n//valid_generator.batch_size is 12678//32\n",
      "test_generator.n//test_generator.batch_size is 4096//32\n",
      "train step size: 8865, validation step size: 396, test step size: 128\n"
     ]
    }
   ],
   "source": [
    "steps_train=train_generator.n//train_generator.batch_size\n",
    "steps_valid=valid_generator.n//valid_generator.batch_size\n",
    "steps_test=test_generator.n//test_generator.batch_size\n",
    "#STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "print('train_generator.n//train_generator.batch_size is {}//{}'.format(train_generator.n, train_generator.batch_size))\n",
    "print('valid_generator.n//valid_generator.batch_size is {}//{}'.format(valid_generator.n, valid_generator.batch_size))\n",
    "print('test_generator.n//test_generator.batch_size is {}//{}'.format(test_generator.n, test_generator.batch_size))\n",
    "print('train step size: {}, validation step size: {}, test step size: {}'.format(steps_train, steps_valid, steps_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models\n",
    "\n",
    "This cell is reused for each tested model. The models are all added to the 'models' library and labeled with a number. See the noted results of each model in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_72 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 31, 31, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 29, 29, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 5,297,162\n",
      "Trainable params: 5,297,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1488\n",
      "Epoch 00001: val_loss improved from inf to 1.00000, saving model to saved_models/weights.best.v7_1.hdf5\n",
      "8865/8865 [==============================] - 3468s 391ms/step - loss: 1.0000 - acc: 0.1488 - val_loss: 1.0000 - val_acc: 0.0317\n",
      "Epoch 2/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1479\n",
      "Epoch 00002: val_loss did not improve from 1.00000\n",
      "8865/8865 [==============================] - 3447s 389ms/step - loss: 1.0000 - acc: 0.1479 - val_loss: 1.0000 - val_acc: 0.0522\n",
      "Epoch 3/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1479\n",
      "Epoch 00003: val_loss did not improve from 1.00000\n",
      "8865/8865 [==============================] - 3458s 390ms/step - loss: 1.0000 - acc: 0.1479 - val_loss: 1.0000 - val_acc: 0.1896\n",
      "Epoch 4/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1472\n",
      "Epoch 00004: val_loss improved from 1.00000 to 1.00000, saving model to saved_models/weights.best.v7_1.hdf5\n",
      "8865/8865 [==============================] - 3442s 388ms/step - loss: 1.0000 - acc: 0.1472 - val_loss: 1.0000 - val_acc: 0.0319\n",
      "Epoch 5/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1472\n",
      "Epoch 00005: val_loss did not improve from 1.00000\n",
      "8865/8865 [==============================] - 3416s 385ms/step - loss: 1.0000 - acc: 0.1472 - val_loss: 1.0000 - val_acc: 0.0371\n",
      "Epoch 6/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1483\n",
      "Epoch 00006: val_loss did not improve from 1.00000\n",
      "8865/8865 [==============================] - 3420s 386ms/step - loss: 1.0000 - acc: 0.1483 - val_loss: 1.0000 - val_acc: 0.0698\n",
      "Epoch 7/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1480\n",
      "Epoch 00007: val_loss did not improve from 1.00000\n",
      "8865/8865 [==============================] - 3426s 386ms/step - loss: 1.0000 - acc: 0.1480 - val_loss: 1.0000 - val_acc: 0.1896\n",
      "Epoch 8/8\n",
      "8864/8865 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.1466\n",
      "Epoch 00008: val_loss did not improve from 1.00000\n",
      "8865/8865 [==============================] - 3424s 386ms/step - loss: 1.0000 - acc: 0.1466 - val_loss: 1.0000 - val_acc: 0.1641\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "\n",
    "# Save the checkpoints to here. Make sure the file name reflects the model version.\n",
    "model_hdf5 = 'saved_models/weights.best.v7_1.hdf5'\n",
    "\n",
    "# Use the same model for each of the steps below. See 'models.py' for details.\n",
    "model_creator = models.create_model_v7\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=model_hdf5, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "train_model = model_creator(show_summary=True)\n",
    "train_model.fit_generator(generator=train_generator,\n",
    "                          steps_per_epoch=steps_train,\n",
    "                          validation_data=valid_generator,\n",
    "                          validation_steps=steps_valid,\n",
    "                          epochs=8,\n",
    "                          callbacks=[checkpointer],\n",
    "                          workers=4\n",
    ")\n",
    "\n",
    "r_trained_orig = train_model.evaluate_generator(generator=valid_generator, steps=steps_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "NOTE: The results below are ONLY for the most recently tested model.\n",
    "\n",
    "<b> Please see the `models.py` file for all of the models and the results for each.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Accuracy of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loss:  1.0000036166171835  Accuracy:  0.16414142\n"
     ]
    }
   ],
   "source": [
    "#print(train_model.metrics_names)\n",
    "#print(r_trained_orig)\n",
    "print('## Loss: ', r_trained_orig[0], ' Accuracy: ', r_trained_orig[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Untrained Model\n",
    "### Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loss:  1.0031691053882241  Accuracy:  0.037597656\n"
     ]
    }
   ],
   "source": [
    "# first test untrained model\n",
    "untrained_test_model = model_creator()\n",
    "\n",
    "score_untrained = untrained_test_model.evaluate_generator(test_generator, steps=steps_test)\n",
    "print('## Loss: ', score_untrained[0], ' Accuracy: ', score_untrained[1])\n",
    "\n",
    "#for i, n in enumerate(test_generator.filenames):\n",
    "#    print('file:', n, ' score: ', scores[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Total test records: 4096\n",
      "## Number of correct: 226\n",
      "## Percent correct: 0.05517578125\n"
     ]
    }
   ],
   "source": [
    "## Testing the untrained model\n",
    "importlib.reload(general_utils)\n",
    "\n",
    "results = general_utils.run_prediction(untrained_test_model, test_generator, steps_test)\n",
    "num_untrained_test_correct = np.count_nonzero(results)\n",
    "print('## Total test records:', len(results))\n",
    "print('## Number of correct:', num_untrained_test_correct)\n",
    "print('## Percent correct:', num_untrained_test_correct/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Trained Model\n",
    "### Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Loss:  1.0000030007213354  Accuracy:  0.034423828\n"
     ]
    }
   ],
   "source": [
    "# next test the trained model\n",
    "trained_test_model = model_creator()\n",
    "trained_test_model.load_weights(model_hdf5)\n",
    "\n",
    "score_trained = trained_test_model.evaluate_generator(test_generator, steps=steps_test)\n",
    "print('## Loss: ', score_trained[0], ' Accuracy: ', score_trained[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of the Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Total test records: 4096\n",
      "## Number of correct: 141\n",
      "## Percent correct: 0.034423828125\n"
     ]
    }
   ],
   "source": [
    "## Testing the trained model\n",
    "importlib.reload(general_utils)\n",
    "\n",
    "results = general_utils.run_prediction(trained_test_model, test_generator, steps_test)\n",
    "num_trained_test_correct = np.count_nonzero(results)\n",
    "print('## Total test records:', len(results))\n",
    "print('## Number of correct:', num_trained_test_correct)\n",
    "print('## Percent correct:', num_trained_test_correct/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the NSynth Checkpoints\n",
    "\n",
    "This is for the next iteration. I made an attempt to use the ckpt files, however, I was unable to use them properly. I believe I will have to apply transfer learning to their trained model, but since I was experimenting with my own and also unable to load their model, I haven't been able to test this yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next test the model trained by NSynth (think I'll have to use a transfer learning approach)\n",
    "\n",
    "###NOTE: Could not figure out how to use the ckpt checkpoint files to either evaluate or even use \n",
    "### . as the basis for transfer learning. I'd like to go back and do this, but could use some advice.\n",
    "\n",
    "#trained_nsynth_test_model = models.create_model_v1()\n",
    "\n",
    "#nsynth_checkpoint = tf.train.load_checkpoint('examples/model_files/model.ckpt-200000') #, latest_filename='model.ckpt-200000.index'\n",
    "#print(nsynth_checkpoint)\n",
    "\n",
    "### results of this was:\n",
    "## NotImplementedError: Streaming restore not supported from name-based checkpoints when graph building. \n",
    "##   File a feature request if this limitation bothers you. \n",
    "##   As a workaround, consider either using tf.train.Checkpoint to load \n",
    "##   name-based checkpoints or enabling eager execution.\n",
    "\n",
    "## I did find this. See link in the comment: https://github.com/tensorflow/magenta/issues/955\n",
    "\n",
    "#path = 'examples/model_files/model.ckpt-200000'\n",
    "\n",
    "#cp = tf.train.Checkpoint(path)\n",
    "\n",
    "\n",
    "#trained_nsynth_test_model.load_weights(path)\n",
    "\n",
    "\n",
    "#r_trained = trained_nsynth_test_model.evaluate_generator(test_generator, steps=STEP_SIZE_TEST)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
